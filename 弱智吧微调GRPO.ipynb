{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPyJWgoNknJ5FOmPwBF1yDB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuqrui/-GRPO-/blob/main/%E5%BC%B1%E6%99%BA%E5%90%A7%E5%BE%AE%E8%B0%83GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "çŸ¥ä¹æ•™ç¨‹https://zhuanlan.zhihu.com/p/23035781247ï¼Œ\n",
        "æ•°æ®é›†é‡‡ç”¨hugingface ruozhibaæ•°æ®é›†"
      ],
      "metadata": {
        "id": "ZFFOVFLN0KFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¬¬1æ­¥**ï¼Œä¸‹è½½å¿…è¦çš„åº“ï¼ˆåœ¨æœ¬åœ°è¿è¡Œå’Œåœ¨colabä¸Šè¿è¡Œæœ‰äº›è®¸å·®åˆ«çš„å“¦ï¼‰"
      ],
      "metadata": {
        "id": "WXG7da_AzMxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "X6k28jRZwYRh"
      },
      "outputs": [],
      "source": [
        "# è·³è¿‡Colabçš„é‡å¯æç¤º\n",
        "import sys; modules = list(sys.modules.keys())\n",
        "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "\n",
        "!pip install unsloth vllm    # å®‰è£…æ ¸å¿ƒåº“unslothï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰å’Œvllmï¼ˆå¿«é€Ÿæ¨ç†ï¼‰\n",
        "!pip install --upgrade pillow  # å‡çº§å›¾åƒå¤„ç†åº“\n",
        "# å¦‚æœæ˜¯æœ¬åœ°è¿è¡Œï¼Œéœ€è¦å®‰è£…diffusers\n",
        "# !pip install diffusers\n",
        "# å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„TRLåº“ï¼ˆæ”¯æŒGRPOç®—æ³•ï¼‰\n",
        "!pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b\n",
        "!pip install sentence-transformers #æ–‡æœ¬ç›¸ä¼¼åº¦æ‰€éœ€çš„åº“"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¬¬2æ­¥**ï¼Œåˆå§‹åŒ–Unslothå’ŒGRPO"
      ],
      "metadata": {
        "id": "IEY_h3ABzKft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)  # ä¸ºGRPOç®—æ³•æ‰“è¡¥ä¸ä»¥åŠ é€Ÿè®­ç»ƒ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Izl_qqy0YBS",
        "outputId": "9cf36816-0627-4587-d646-da515df9f7c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Patching Xformers to fix some performance issues.\n",
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¬¬3æ­¥**ï¼ŒåŠ è½½é¢„è®­ç»ƒå¤§æ¨¡å‹ï¼Œæˆ‘è¿™é‡Œä½¿ç”¨Qwen2.5ï¼Œä½¿ç”¨loraè¿›è¡Œå¾®è°ƒ"
      ],
      "metadata": {
        "id": "Xh8MQY4ay8cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "\n",
        "max_seq_length = 1024   # æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦\n",
        "lora_rank = 64         # LoRAçš„ç§©ï¼Œå€¼è¶Šå¤§æ¨¡å‹èƒ½åŠ›è¶Šå¼ºä½†é€Ÿåº¦è¶Šæ…¢\n",
        "\n",
        "# ä»HuggingFaceåŠ è½½Qwen2.5-3B-Instructæ¨¡å‹\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,       # 4ä½é‡åŒ–åŠ è½½ä»¥èŠ‚çœæ˜¾å­˜\n",
        "    fast_inference = True,     # å¯ç”¨vLLMåŠ é€Ÿæ¨ç†\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.5,  # GPUæ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆé™ä½å¯ç¼“è§£OOMï¼‰\n",
        ")\n",
        "\n",
        "# ä¸ºæ¨¡å‹æ·»åŠ LoRAé€‚é…å™¨\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,            # LoRAç§©\n",
        "    target_modules = [         # åº”ç”¨LoRAçš„ç›®æ ‡æ¨¡å—\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank,    # LoRAç¼©æ”¾ç³»æ•°\n",
        "    use_gradient_checkpointing = \"unsloth\",  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥æ”¯æŒé•¿åºåˆ—\n",
        "    random_state = 666,       # éšæœºç§å­\n",
        ")"
      ],
      "metadata": {
        "id": "YZvbljSw1Ea4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹çš„è¾“å‡º"
      ],
      "metadata": {
        "id": "lJJxt4H7zqvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"è¯·å†™ä¸€é¦–æ˜¥å¤©çš„è¯—ï¼š\"\n",
        "inputs = tokenizer(text, return_tensors='pt').to('cuda')"
      ],
      "metadata": {
        "id": "E2Qkp0pc5LOZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = model(**inputs)\n",
        "FastLanguageModel.for_inference(model)\n",
        "outputs = model.generate(\n",
        "    input_ids = inputs['input_ids'], # æ˜¾å¼ä¼ é€’ input_ids\n",
        "    attention_mask = inputs['attention_mask'], # **æ˜¾å¼ä¼ é€’ attention_mask**\n",
        "    max_length=100,      # è®¾ç½®æœ€å¤§ç”Ÿæˆé•¿åº¦ (åŒ…æ‹¬ prompt å’Œç”Ÿæˆçš„æ–‡æœ¬)\n",
        "    num_beams=1,         # **ç¦ç”¨é›†æŸæœç´¢ï¼Œè®¾ç½® num_beams=1**\n",
        "    temperature=0.7,     # é‡‡æ ·æ¸©åº¦ï¼Œæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ (å¯é€‰ï¼Œå€¼è¶Šä½è¶Šç¡®å®šï¼Œè¶Šé«˜è¶Šéšæœº)\n",
        "    top_k=50,            # Top-k é‡‡æ ·ï¼Œé™åˆ¶å€™é€‰è¯çš„èŒƒå›´ (å¯é€‰)\n",
        "    top_p=0.95,          # Top-p é‡‡æ · (nucleus sampling)ï¼Œå¦ä¸€ç§é™åˆ¶å€™é€‰è¯çš„æ–¹æ³• (å¯é€‰)\n",
        "    repetition_penalty=1.2, # é‡å¤æƒ©ç½šï¼Œå‡å°‘ç”Ÿæˆé‡å¤æ–‡æœ¬çš„å¯èƒ½æ€§ (å¯é€‰)\n",
        "    no_repeat_ngram_size=3, #  é˜²æ­¢ç”Ÿæˆ n-gram é‡å¤ (å¯é€‰ï¼Œä¾‹å¦‚é˜²æ­¢ 3-gram é‡å¤)\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Prompt:\\n\", inputs)\n",
        "print(\"\\nGenerated Poem:\\n\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVdzqYHx66dL",
        "outputId": "8642c346-b9ce-4501-c724-1af17d7b2f97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            " {'input_ids': tensor([[ 14880,  61443, 108462, 105303,   9370, 100045,   5122]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
            "\n",
            "Generated Poem:\n",
            " è¯·å†™ä¸€é¦–æ˜¥å¤©çš„è¯—ï¼š æ˜¥å¤©ï¼Œä½ æ¥äº†\n",
            "ä¸‡ç‰©å¤è‹ï¼Œæ˜¥æ„æµ“ï¼Œ\n",
            "ç»¿å¶å«©èŠ½ç ´åœŸè€Œå‡ºã€‚\n",
            "é¸Ÿå„¿æ¬¢æ­Œåœ¨æå¤´ï¼Œ\n",
            "èŠ±é¦™å››æº¢ï¼Œèœœèœ‚å¿™ç¢Œã€‚\n",
            "\n",
            "æ˜¥é£è½»æ‹‚è¿‡ç”°é‡ï¼Œ\n",
            "éº¦è‹—é’ç¿ ï¼Œç”Ÿæœºå‹ƒå‹ƒã€‚\n",
            "å°æºªæ½ºæ½ºæµæ·Œç€ï¼Œ\n",
            "é±¼å„¿å¬‰æˆï¼Œæ¬¢å¿«æ— æ¯”ã€‚\n",
            "\n",
            "å­©å­ä»¬ç©¿ç€æ–°è¡£è£³ï¼Œ\n",
            "è¿½é€è´è¶ï¼Œåœ¨è‰åœ°ä¸Šå¥”è·‘ã€‚\n",
            "è€äººä»¬ååœ¨æ ‘è«ä¸‹ï¼Œ\n",
            "äº«å—é˜³å…‰ï¼Œè°ˆç¬‘é£\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¬¬4æ­¥**ï¼Œå®šä¹‰å›ç­”æ ¼å¼"
      ],
      "metadata": {
        "id": "YwnqZzpX0lra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "è¯·ä½¿ç”¨ä¸­æ–‡æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”é—®é¢˜:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5VY1btgPPOhj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¬¬5æ­¥**ï¼Œhugingfaceå¼±æ™ºå§-æ•°æ®åŠ è½½åŠå…¶é¢„å¤„ç†https://huggingface.co/datasets/LooksJuicy/ruozhiba"
      ],
      "metadata": {
        "id": "dJ4cwcpD0bP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "import json\n",
        "\n",
        "def load_custom_dataset(ds) -> Dataset:\n",
        "    # å¤„ç†æ•°æ®ä¸ºç¬¦åˆè®­ç»ƒæ ¼å¼\n",
        "    processed_data = []\n",
        "    for item in ds['train']:\n",
        "      print(item)\n",
        "      instruction = item[\"instruction\"]\n",
        "      output = item[\"output\"]\n",
        "\n",
        "      # è®¾å®š prompt æ ¼å¼ï¼ˆç¬¦åˆ chat è®­ç»ƒæ ¼å¼ï¼‰\n",
        "      prompt = [\n",
        "          {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "          {\"role\": \"user\", \"content\": instruction}\n",
        "      ]\n",
        "\n",
        "      processed_data.append({\"prompt\": prompt, \"answer\": output})\n",
        "\n",
        "    # è½¬æ¢ä¸º Hugging Face Dataset\n",
        "    dataset = Dataset.from_list(processed_data)\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "6wc9gAkOVtvs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"LooksJuicy/ruozhiba\")\n",
        "print(ds)\n",
        "# åŠ è½½æ•°æ®é›†\n",
        "dataset = load_custom_dataset(ds)\n",
        "dataset"
      ],
      "metadata": {
        "id": "E8nlImLTZ0aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¬¬6æ­¥**ï¼Œå®šä¹‰å„ç§å¥–åŠ±å‡½æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨GRPOè¿™ä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè‡ªç„¶è¦è®¾ç½®ä¸€ä¸‹rewardã€‚å› ä¸ºæˆ‘ä»¬å¸Œæœ›æ¨¡å‹æ¨ç†å¼±æ™ºå§è¿™ä¸ªä»»åŠ¡ï¼Œå±äºä¸€ä¸ªå¼€æ”¾é—®ç­”çš„ä»»åŠ¡ï¼Œä¸åƒåšæ•°å­¦é¢˜ä¸€æ ·æœ‰ä¸€ä¸ªæ­»æ¿çš„ç­”æ¡ˆï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦åŸºäºè¯­ä¹‰çš„ç›¸ä¼¼åº¦ç»™äºˆä¸åŒçš„å¥–åŠ±ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦è®©æ¨¡å‹ä¿æŒä¸€å®šçš„å›ç­”æ ¼å¼ï¼Œå¦‚æœå›ç­”æ ¼å¼æ­£ç¡®ï¼Œä¹Ÿå¥–åŠ±ä¸€æœµå°çº¢èŠ±ã€‚"
      ],
      "metadata": {
        "id": "DQq9bJJ61ahe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "#åŠ è½½ Sentence Transformers æ¨¡å‹\n",
        "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "text1, text2 = 'æˆ‘å–œæ¬¢çŒ«','æˆ‘å¾ˆå–œæ¬¢ç‹—'\n",
        "\n",
        "similaritie = util.cos_sim(semantic_model.encode(text1), semantic_model.encode(text2))\n",
        "print(semantic_model.encode(text2).shape)\n",
        "print(similaritie)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h956AzkGVm20",
        "outputId": "51e88b66-62f3-48f4-cdf2-65841dce144a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(384,)\n",
            "tensor([[0.9953]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#è¯­ä¹‰ç›¸ä¼¼åº¦å¥–åŠ±\n",
        "def semantic_similarity_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'].strip() for completion in completions]\n",
        "    answer = [a.strip() for a in answer]\n",
        "\n",
        "    # è®¡ç®—ç›¸ä¼¼åº¦\n",
        "    similarities = util.cos_sim(semantic_model.encode(responses), semantic_model.encode(answer))\n",
        "    print(similaritie.shape)\n",
        "    rewards = []\n",
        "    for sim in similarities.diagonal().tolist():  # å–å¯¹è§’çº¿ä¸Šçš„å€¼ï¼ˆå•ä¸ªæ ·æœ¬çš„ç›¸ä¼¼åº¦ï¼‰\n",
        "        if sim > 0.9:\n",
        "            rewards.append(2.0)  # éå¸¸æ¥è¿‘\n",
        "        elif sim > 0.7:\n",
        "            rewards.append(1.5)  # ç›¸å…³æ€§è¾ƒé«˜\n",
        "        elif sim > 0.5:\n",
        "            rewards.append(1.0)  # å¯èƒ½éƒ¨åˆ†æ­£ç¡®\n",
        "        else:\n",
        "            rewards.append(0.0)  # ç›¸å…³æ€§ä½\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# ä¸¥æ ¼æ ¼å¼å¥–åŠ±ï¼šå¿…é¡»å®Œå…¨åŒ¹é… <reasoning>...</reasoning><answer>...</answer>\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    return [0.5 if re.match(pattern, r) else 0.0 for r in responses]\n",
        "\n",
        "# è½¯æ ¼å¼å¥–åŠ±ï¼šåªéœ€åŒ…å« <reasoning> å’Œ <answer> éƒ¨åˆ†\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    return [0.5 if re.search(pattern, r) else 0.0 for r in responses]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ],
      "metadata": {
        "id": "mR0F9hTGcFxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¬¬7æ­¥ï¼š**é…ç½®GRPOå‚æ•°"
      ],
      "metadata": {
        "id": "AwxGMxBam1bE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True,   # ä½¿ç”¨vLLMåŠ é€Ÿæ¨ç†\n",
        "    learning_rate = 5e-6, # å­¦ä¹ ç‡\n",
        "    adam_beta1 = 0.9,   # Adamä¼˜åŒ–å™¨å‚æ•°\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,  # æƒé‡è¡°å‡\n",
        "    warmup_ratio = 0.1,  # å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹\n",
        "    lr_scheduler_type = \"cosine\",  # å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥\n",
        "    optim = \"adamw_8bit\",      # 8ä½Adamä¼˜åŒ–å™¨\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),   # æ ¹æ®ç¡¬ä»¶æ”¯æŒé€‰æ‹©ç²¾åº¦\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 1,  # batch size,ä½ è®¡ç®—èµ„æºå¤Ÿçš„è¯ï¼Œå¯ä»¥è®¾ç½®é«˜ä¸€ç‚¹\n",
        "    gradient_accumulation_steps = 1, # ç´¯è®¡1æ­¥åæ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œæ—¶é—´æ¢ç©ºé—´ï¼Œå°†batch_sizeåˆ†ä¸ºstepsä¸ªmini_batchè¿›è¡Œæ¢¯åº¦ç´¯ç§¯ï¼Œå†æ›´æ–°å‚æ•°\n",
        "    num_generations = 8,  # æ¯æ¬¡ç”Ÿæˆçš„å€™é€‰æ•°\n",
        "    max_prompt_length = 256,  # è¾“å…¥æœ€å¤§é•¿åº¦\n",
        "    max_completion_length = 200,  # ç”Ÿæˆæœ€å¤§é•¿åº¦\n",
        "\n",
        "    max_steps = 200,    # æœ€å¤§è®­ç»ƒæ­¥æ•°\n",
        "    save_steps = 50,   # ä¿å­˜é—´éš”\n",
        "    max_grad_norm = 0.1,   # æ¢¯åº¦è£å‰ªé˜ˆå€¼\n",
        "    report_to = \"none\",\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "c_2PwSI8m9AF",
        "outputId": "06989b14-2644-4611-bed2-a89fb2aaf27a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'trl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0a46fb5814c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGRPOConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRPOTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m training_args = GRPOConfig(\n\u001b[1;32m      3\u001b[0m     \u001b[0muse_vllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# ä½¿ç”¨vLLMåŠ é€Ÿæ¨ç†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# å­¦ä¹ ç‡\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0madam_beta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# Adamä¼˜åŒ–å™¨å‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¬¬8æ­¥**ï¼Œé…ç½®è®­ç»ƒå™¨ï¼Œå¼€å§‹è®­ç»ƒ"
      ],
      "metadata": {
        "id": "K3zjdz9RvalB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [   # å¥–åŠ±å‡½æ•°åˆ—è¡¨\n",
        "        xmlcount_reward_func,   # XMLç»“æ„å¥–åŠ±\n",
        "        soft_format_reward_func,  # å®½æ¾æ ¼å¼å¥–åŠ±\n",
        "        strict_format_reward_func,   # ä¸¥æ ¼æ ¼å¼å¥–åŠ±\n",
        "        semantic_similarity_reward_func  # è¯­ä¹‰ç›¸ä¼¼å¥–åŠ±\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train() #å¯åŠ¨è®­ç»ƒ"
      ],
      "metadata": {
        "id": "8YhLrlxMvo1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¬¬9æ­¥**ï¼Œä¿å­˜lora"
      ],
      "metadata": {
        "id": "8pCzdNtxv8Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ],
      "metadata": {
        "id": "XKofW4-gv_F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¬¬10æ­¥**ï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæµ‹è¯•è‹¥è‡³å‘è¨€ï¼Œçœ‹çœ‹ä»–çš„æ¨ç†è¿‡ç¨‹ã€‚"
      ],
      "metadata": {
        "id": "d_8tjq8LvQgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import SamplingParams\n",
        "def test_ruozhi(prompt):\n",
        "  text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : prompt},\n",
        "  ], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "\n",
        "  sampling_params = SamplingParams(\n",
        "    temperature = 0.8,  # ç”Ÿæˆæ¸©åº¦ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰\n",
        "    top_p = 0.95,   # æ ¸é‡‡æ ·é˜ˆå€¼\n",
        "    max_tokens = 1024, # æœ€å¤§ç”Ÿæˆtokenæ•°\n",
        "  )\n",
        "  output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        "  )[0].outputs[0].text\n",
        "  print(\"\\n\")\n",
        "  print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "0nV9S6LbvSv1",
        "outputId": "f620c9aa-ee2e-4f2f-8bf9-a64301c67193"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vllm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bf1459c18fdf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSamplingParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_ruozhi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   text = tokenizer.apply_chat_template([\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mSYSTEM_PROMPT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vllm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ruozhi(\"æˆ‘æŠŠæ¼‚ç™½æ°´å–äº†æˆ‘çš„çš®è‚¤ä¼šä¸ä¼šå˜ç™½ï¼Ÿ\")"
      ],
      "metadata": {
        "id": "IvhTzdx5vdSB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}