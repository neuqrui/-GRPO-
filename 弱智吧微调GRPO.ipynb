{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPyJWgoNknJ5FOmPwBF1yDB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuqrui/-GRPO-/blob/main/%E5%BC%B1%E6%99%BA%E5%90%A7%E5%BE%AE%E8%B0%83GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "知乎教程https://zhuanlan.zhihu.com/p/23035781247，\n",
        "数据集采用hugingface ruozhiba数据集"
      ],
      "metadata": {
        "id": "ZFFOVFLN0KFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**第1步**，下载必要的库（在本地运行和在colab上运行有些许差别的哦）"
      ],
      "metadata": {
        "id": "WXG7da_AzMxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "X6k28jRZwYRh"
      },
      "outputs": [],
      "source": [
        "# 跳过Colab的重启提示\n",
        "import sys; modules = list(sys.modules.keys())\n",
        "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "\n",
        "!pip install unsloth vllm    # 安装核心库unsloth（加速训练）和vllm（快速推理）\n",
        "!pip install --upgrade pillow  # 升级图像处理库\n",
        "# 如果是本地运行，需要安装diffusers\n",
        "# !pip install diffusers\n",
        "# 安装特定版本的TRL库（支持GRPO算法）\n",
        "!pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b\n",
        "!pip install sentence-transformers #文本相似度所需的库"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**第2步**，初始化Unsloth和GRPO"
      ],
      "metadata": {
        "id": "IEY_h3ABzKft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)  # 为GRPO算法打补丁以加速训练"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Izl_qqy0YBS",
        "outputId": "9cf36816-0627-4587-d646-da515df9f7c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Patching Xformers to fix some performance issues.\n",
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**第3步**，加载预训练大模型，我这里使用Qwen2.5，使用lora进行微调"
      ],
      "metadata": {
        "id": "Xh8MQY4ay8cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "\n",
        "max_seq_length = 1024   # 模型支持的最大序列长度\n",
        "lora_rank = 64         # LoRA的秩，值越大模型能力越强但速度越慢\n",
        "\n",
        "# 从HuggingFace加载Qwen2.5-3B-Instruct模型\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,       # 4位量化加载以节省显存\n",
        "    fast_inference = True,     # 启用vLLM加速推理\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.5,  # GPU显存利用率（降低可缓解OOM）\n",
        ")\n",
        "\n",
        "# 为模型添加LoRA适配器\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,            # LoRA秩\n",
        "    target_modules = [         # 应用LoRA的目标模块\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank,    # LoRA缩放系数\n",
        "    use_gradient_checkpointing = \"unsloth\",  # 启用梯度检查点以支持长序列\n",
        "    random_state = 666,       # 随机种子\n",
        ")"
      ],
      "metadata": {
        "id": "YZvbljSw1Ea4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试预训练模型的输出"
      ],
      "metadata": {
        "id": "lJJxt4H7zqvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"请写一首春天的诗：\"\n",
        "inputs = tokenizer(text, return_tensors='pt').to('cuda')"
      ],
      "metadata": {
        "id": "E2Qkp0pc5LOZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = model(**inputs)\n",
        "FastLanguageModel.for_inference(model)\n",
        "outputs = model.generate(\n",
        "    input_ids = inputs['input_ids'], # 显式传递 input_ids\n",
        "    attention_mask = inputs['attention_mask'], # **显式传递 attention_mask**\n",
        "    max_length=100,      # 设置最大生成长度 (包括 prompt 和生成的文本)\n",
        "    num_beams=1,         # **禁用集束搜索，设置 num_beams=1**\n",
        "    temperature=0.7,     # 采样温度，控制生成文本的随机性 (可选，值越低越确定，越高越随机)\n",
        "    top_k=50,            # Top-k 采样，限制候选词的范围 (可选)\n",
        "    top_p=0.95,          # Top-p 采样 (nucleus sampling)，另一种限制候选词的方法 (可选)\n",
        "    repetition_penalty=1.2, # 重复惩罚，减少生成重复文本的可能性 (可选)\n",
        "    no_repeat_ngram_size=3, #  防止生成 n-gram 重复 (可选，例如防止 3-gram 重复)\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Prompt:\\n\", inputs)\n",
        "print(\"\\nGenerated Poem:\\n\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVdzqYHx66dL",
        "outputId": "8642c346-b9ce-4501-c724-1af17d7b2f97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            " {'input_ids': tensor([[ 14880,  61443, 108462, 105303,   9370, 100045,   5122]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
            "\n",
            "Generated Poem:\n",
            " 请写一首春天的诗： 春天，你来了\n",
            "万物复苏，春意浓，\n",
            "绿叶嫩芽破土而出。\n",
            "鸟儿欢歌在枝头，\n",
            "花香四溢，蜜蜂忙碌。\n",
            "\n",
            "春风轻拂过田野，\n",
            "麦苗青翠，生机勃勃。\n",
            "小溪潺潺流淌着，\n",
            "鱼儿嬉戏，欢快无比。\n",
            "\n",
            "孩子们穿着新衣裳，\n",
            "追逐蝴蝶，在草地上奔跑。\n",
            "老人们坐在树荫下，\n",
            "享受阳光，谈笑风\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**第4步**，定义回答格式"
      ],
      "metadata": {
        "id": "YwnqZzpX0lra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "请使用中文按以下格式回答问题:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5VY1btgPPOhj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**第5步**，hugingface弱智吧-数据加载及其预处理https://huggingface.co/datasets/LooksJuicy/ruozhiba"
      ],
      "metadata": {
        "id": "dJ4cwcpD0bP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "import json\n",
        "\n",
        "def load_custom_dataset(ds) -> Dataset:\n",
        "    # 处理数据为符合训练格式\n",
        "    processed_data = []\n",
        "    for item in ds['train']:\n",
        "      print(item)\n",
        "      instruction = item[\"instruction\"]\n",
        "      output = item[\"output\"]\n",
        "\n",
        "      # 设定 prompt 格式（符合 chat 训练格式）\n",
        "      prompt = [\n",
        "          {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "          {\"role\": \"user\", \"content\": instruction}\n",
        "      ]\n",
        "\n",
        "      processed_data.append({\"prompt\": prompt, \"answer\": output})\n",
        "\n",
        "    # 转换为 Hugging Face Dataset\n",
        "    dataset = Dataset.from_list(processed_data)\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "6wc9gAkOVtvs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"LooksJuicy/ruozhiba\")\n",
        "print(ds)\n",
        "# 加载数据集\n",
        "dataset = load_custom_dataset(ds)\n",
        "dataset"
      ],
      "metadata": {
        "id": "E8nlImLTZ0aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**第6步**，定义各种奖励函数，我们使用GRPO这个强化学习算法，自然要设置一下reward。因为我们希望模型推理弱智吧这个任务，属于一个开放问答的任务，不像做数学题一样有一个死板的答案，因此我们需要基于语义的相似度给予不同的奖励。此外，还需要让模型保持一定的回答格式，如果回答格式正确，也奖励一朵小红花。"
      ],
      "metadata": {
        "id": "DQq9bJJ61ahe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "#加载 Sentence Transformers 模型\n",
        "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "text1, text2 = '我喜欢猫','我很喜欢狗'\n",
        "\n",
        "similaritie = util.cos_sim(semantic_model.encode(text1), semantic_model.encode(text2))\n",
        "print(semantic_model.encode(text2).shape)\n",
        "print(similaritie)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h956AzkGVm20",
        "outputId": "51e88b66-62f3-48f4-cdf2-65841dce144a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(384,)\n",
            "tensor([[0.9953]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#语义相似度奖励\n",
        "def semantic_similarity_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'].strip() for completion in completions]\n",
        "    answer = [a.strip() for a in answer]\n",
        "\n",
        "    # 计算相似度\n",
        "    similarities = util.cos_sim(semantic_model.encode(responses), semantic_model.encode(answer))\n",
        "    print(similaritie.shape)\n",
        "    rewards = []\n",
        "    for sim in similarities.diagonal().tolist():  # 取对角线上的值（单个样本的相似度）\n",
        "        if sim > 0.9:\n",
        "            rewards.append(2.0)  # 非常接近\n",
        "        elif sim > 0.7:\n",
        "            rewards.append(1.5)  # 相关性较高\n",
        "        elif sim > 0.5:\n",
        "            rewards.append(1.0)  # 可能部分正确\n",
        "        else:\n",
        "            rewards.append(0.0)  # 相关性低\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# 严格格式奖励：必须完全匹配 <reasoning>...</reasoning><answer>...</answer>\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    return [0.5 if re.match(pattern, r) else 0.0 for r in responses]\n",
        "\n",
        "# 软格式奖励：只需包含 <reasoning> 和 <answer> 部分\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    return [0.5 if re.search(pattern, r) else 0.0 for r in responses]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ],
      "metadata": {
        "id": "mR0F9hTGcFxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**第7步：**配置GRPO参数"
      ],
      "metadata": {
        "id": "AwxGMxBam1bE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True,   # 使用vLLM加速推理\n",
        "    learning_rate = 5e-6, # 学习率\n",
        "    adam_beta1 = 0.9,   # Adam优化器参数\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,  # 权重衰减\n",
        "    warmup_ratio = 0.1,  # 学习率预热比例\n",
        "    lr_scheduler_type = \"cosine\",  # 学习率调度策略\n",
        "    optim = \"adamw_8bit\",      # 8位Adam优化器\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),   # 根据硬件支持选择精度\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 1,  # batch size,你计算资源够的话，可以设置高一点\n",
        "    gradient_accumulation_steps = 1, # 累计1步后更新一次参数，时间换空间，将batch_size分为steps个mini_batch进行梯度累积，再更新参数\n",
        "    num_generations = 8,  # 每次生成的候选数\n",
        "    max_prompt_length = 256,  # 输入最大长度\n",
        "    max_completion_length = 200,  # 生成最大长度\n",
        "\n",
        "    max_steps = 200,    # 最大训练步数\n",
        "    save_steps = 50,   # 保存间隔\n",
        "    max_grad_norm = 0.1,   # 梯度裁剪阈值\n",
        "    report_to = \"none\",\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "c_2PwSI8m9AF",
        "outputId": "06989b14-2644-4611-bed2-a89fb2aaf27a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'trl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0a46fb5814c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGRPOConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRPOTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m training_args = GRPOConfig(\n\u001b[1;32m      3\u001b[0m     \u001b[0muse_vllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# 使用vLLM加速推理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 学习率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0madam_beta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# Adam优化器参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**第8步**，配置训练器，开始训练"
      ],
      "metadata": {
        "id": "K3zjdz9RvalB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [   # 奖励函数列表\n",
        "        xmlcount_reward_func,   # XML结构奖励\n",
        "        soft_format_reward_func,  # 宽松格式奖励\n",
        "        strict_format_reward_func,   # 严格格式奖励\n",
        "        semantic_similarity_reward_func  # 语义相似奖励\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train() #启动训练"
      ],
      "metadata": {
        "id": "8YhLrlxMvo1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**第9步**，保存lora"
      ],
      "metadata": {
        "id": "8pCzdNtxv8Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ],
      "metadata": {
        "id": "XKofW4-gv_F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**第10步**，使用训练好的模型，测试若至发言，看看他的推理过程。"
      ],
      "metadata": {
        "id": "d_8tjq8LvQgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import SamplingParams\n",
        "def test_ruozhi(prompt):\n",
        "  text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : prompt},\n",
        "  ], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "\n",
        "  sampling_params = SamplingParams(\n",
        "    temperature = 0.8,  # 生成温度（越高越随机）\n",
        "    top_p = 0.95,   # 核采样阈值\n",
        "    max_tokens = 1024, # 最大生成token数\n",
        "  )\n",
        "  output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        "  )[0].outputs[0].text\n",
        "  print(\"\\n\")\n",
        "  print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "0nV9S6LbvSv1",
        "outputId": "f620c9aa-ee2e-4f2f-8bf9-a64301c67193"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vllm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bf1459c18fdf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSamplingParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_ruozhi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   text = tokenizer.apply_chat_template([\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mSYSTEM_PROMPT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vllm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ruozhi(\"我把漂白水喝了我的皮肤会不会变白？\")"
      ],
      "metadata": {
        "id": "IvhTzdx5vdSB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}